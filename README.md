## 22.11.23, 24.11.23

Прочитал задание и начал разбор с попыток запуска приложения. Попутно оформлял README.md. 

Довольно быстро определился с ключами запуска, потом с помощью strace выяснил, какие файлы и папки ожидает приложение. После запуска нашел порт, который оно слушает.

Собрал Dockerfile и docker-compose для локального запуска. Включил в него `postgresql` и `adminer`. Сделал init-контейнер для заполнения БД. Добавил `nginx`. 

Тестировал этот стек примерно пару дней, наблюдая за работой. Попробовал запустить две копии приложения одновременно. Падали они тоже одновременно...

(Итоговый вариант локального стека [здесь](./build/docker-compose.yml))

После этого стала вырисовываться требующаяся архитектура:

- postgresql
- две docker-ноды (docker-compose) для запуска приложения
- nginx в качестве балансера нагрузки
- nginx (еще один?) в качестве кэширующего прокси

## 27.11.23

Начал делать terraform-инфраструктуру в yandex cloud. У меня уже был курс с [дипломным проектом](https://github.com/Kraktorist/build-stack), где я имел возможность потестировать разные подходы, отличавшиеся от тех, которые применял в работе.

Инфраструктуру опиcывал в виде единого yaml файла [config.yaml](./envs/dev/config.yaml), подающегося на вход terraform виде переменной. На выходе получал файл inventory.yaml, содержащий ansible inventory. Я уже знал, что такой подход хорошо работает, поскольку добавлять и удалять ресурсы можно простым редактированием yaml.

Не учел, что грант покрывает лишь лимитированный набор ресурсов. Например, хотел попробовать использовать managed postgresql, но он оказался недоступен. 

В конечном проекте yandex container registry не использовал, загрузив образ на hub.docker.com

## 28.11.23

Реализовал установку `postgresql` с помощью `ansible` и импорт базы данных. Это позволило собрать полуработающую схему, где использовалась внешняя БД, а всё остальное работало в docker-compose.

Понял, что docker-compose не управляет unhealthy контейнерами, добавил сторожа в виде вспомогательного контейнера, рестартующего нездоровые контейнеры.

Реализовал установку nginx.

## 29.11.23

Работал над ошибками, возникающими в тестах. Делал [скриншоты](./status/) страницы отчетов, чтобы отслеживать свой прогресс.

Двумя наиболее времязатратными проблемами оказались следующие:

- **/api/session** запрос работал слишком долго. Посмотрел вывод этого http запроса, понял, что здесь вероятно используется медленный sql-запрос. Сам запрос попался мне в логе ошибок postgresql с текстом, что клиент не дождался ответа. Изначально запрос выполнялся более 30 секунд. Чтобы его ускорить, я просмотрел план запроса и пошел сразу по трем направлениям.
  1. Поместил БД на SSD диск.
  2. Посмотрел настройки сервера, понял, что используются дефолтные буферы, и увеличил значения `work_mem` и `shared_buffers`.
  3. Посмотрел индексы в БД и добавил дефолтные по id и по полям, использующимся для order by.

- **/long_dummy** кэширование. Вообще вопросов кэширования до этого не касался и предполагал, что здесь придется использовать какую-нибудь сложную схему с применением стороннего софта. В конце концов разобрался с настройками nginx.

## 30.11.23 

1. Закончил настройку кэширования.
2. Сделал https на самоподписном сертификате. Думал привязать домен и выпустить сертификат от let's encrypt, но из-за публичного динамического IP, меняющегося при каждом развертывании окружения, было бы тратой времени ждать обновления DNS записей.
3. Поработал над автоматизацией. Делать полноценный пайплайн не стал, а просто реализовал ./install.sh скрипт, которому можно передать имя окружения (то есть папки из envs/) и действие `apply` или `destroy`. За одну команду выполняется полная установка или удаление соответствующего окружения. Такой автоматизации достаточно, чтобы минут за 10-15 поднять пару независимых окружений и работать над одним из них, пока на втором идет автотест.
4. Начал работать над мониторингом. Посчитал, что `prometheus` и `grafana` будет достаточно для решения задачи, однако не нашел внятных экспортеров, показывающих метрики по эндпоинтам. В итоговое решение включен `nginx-prometheus-exporter` и дашборда к нему.

## 01.11.23 Lessons Learned

Проанализировал свое решение. Нашел ошибки в дизайне.

1. Выход из строя одной из нод bingo приведет к необходимости ее _ручного редеплоя_, что уже не гарантирует быстрого восстановления. 
   
   Следовало сделать node group с cloud-init скриптом, который бы включал какое-то service discovery решение по поиску собственного конфига. 

   Следовало добавить NLB для этой node group, чтобы прикрывать выход из строя одной из нод.

2. Хост `nginx` - единая точка отказа.

    Здесь также следовало подумать над реализацией решения на основе пула nginx серверов, прикрытого балансировщиком. 

    Вероятно при этом возникнут проблемы с теми запросами, которые требуют кэширования. Тогда можно было бы попробовать active-passive схему, когда работает только одна нода и один кэш одновременно.

Итоговая архитектура должна была получиться примерно такой:

```
                       / <--> NGINX \               / <--> BINGO \
REQUESTS <--> NLB <-->   node_group   <--> NLB <-->   node_group   <--> POSTGRESQL
                       \ <--> NGINX /               \ <--> BINGO /
```

Неправильно расставил приоритеты. Список зеленых галочек в автотесте плохо отражает реальные цели, описанные в документе. Следовало сконцентрироваться на архитектуре решения, отказоустойчивости и механизмах self healing. Следовало продумать серию гипотетических cлучаев отказа каждого звена в архитектуре, после чего попробовать их реализовать.


